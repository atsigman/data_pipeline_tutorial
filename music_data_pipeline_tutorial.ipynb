{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHbHTkIAY4Ad55l08/BOTA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atsigman/data_pipeline_tutorial/blob/main/music_data_pipeline_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Music Data Pipeline Tutorial**"
      ],
      "metadata": {
        "id": "BXVWyjWJ5fOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Installation Steps**"
      ],
      "metadata": {
        "id": "sJkjwa7icgb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, clone the tutorial repo and install the music_data_pipeline package:"
      ],
      "metadata": {
        "id": "JA8uML7cKlUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvQYiV0_7GNk"
      },
      "outputs": [],
      "source": [
        "!pip install  git+https://github.com/atsigman/data_pipeline_tutorial.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, download text and audio data (mounting the Google drive does not always work):"
      ],
      "metadata": {
        "id": "Uuk5WXZaKz2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "!gdown 1W2V7YbwBSfeECnda0bVFRmR47xmaZYxr -O /content/dpt_data.zip"
      ],
      "metadata": {
        "id": "EOsZI9QV_Ucz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the data archive and save to the appropriate subdirectory:"
      ],
      "metadata": {
        "id": "wuhSenbKLJD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "LpMGK33yF6vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ZIP_PATH = \"/content/dpt_data.zip\"   # location of the zip in Colab VM\n",
        "EXTRACT_TO = \"/content\"\n",
        "DATA_DIR = \"/content/data\"\n",
        "SUBDIR_NAME = os.path.basename(ZIP_PATH)[:-4]"
      ],
      "metadata": {
        "id": "RO30IUxlGBN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip_file() -> None:\n",
        "  \"\"\"\n",
        "  Unzips archive to target directory.\n",
        "  \"\"\"\n",
        "  os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "  # Unzip:\n",
        "  with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_TO)\n",
        "\n",
        "  # Rename subdir to \"data\":\n",
        "  if SUBDIR_NAME != \"data\":\n",
        "    ORIGINAL_DIR = os.path.join(EXTRACT_TO, SUBDIR_NAME)\n",
        "    if os.path.exists(DATA_DIR):\n",
        "      import shutil\n",
        "      shutil.rmtree(DATA_DIR)  # remove if already exists\n",
        "      os.rename(ORIGINAL_DIR, DATA_DIR)\n",
        "\n",
        "  print(f\"âœ… Audio and text metadata extracted to: {DATA_DIR}\")"
      ],
      "metadata": {
        "id": "yuoJXn0LDtJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_file()"
      ],
      "metadata": {
        "id": "rFnFtrpQGcBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports:\n",
        "\n",
        "import ast\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import uuid\n",
        "\n",
        "import torch\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "from torchaudio.transforms import MFCC\n",
        "\n",
        "from music_data_pipeline.audio_dataset import AudioDataset\n",
        "from music_data_pipeline.util.pipeline_utils import (\n",
        "  validate_prune_data,\n",
        "  find_similar_audio,\n",
        "  add_silent_regions,\n",
        "  chunk_audio,\n",
        "  tokenize_metadata,\n",
        "  extract_blacklisted_genres,\n",
        ")"
      ],
      "metadata": {
        "id": "IHf7h8h4GpaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to explore the dataset and construct a data pipeline!\n",
        "\n",
        "# **I. Data Preprocessing Pipeline**"
      ],
      "metadata": {
        "id": "QwKjgo3cLcTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this module will be to analyse and preprocess the audio data and text metadata.\n",
        "\n",
        "As we shall see, it may be necessary to a) prune the metadata in the event of invalid entries, b) add metadata, and/or c) generate new audio files.\n",
        "\n",
        "Ultimately, the input data CSV will be converted to a dictionary, which will be saved as a JSON in the `/content/data` directory.\n",
        "\n",
        "(In the \"real world\", this data would be stored to a DB, but for the sake of simplicity, we will just serialise it to a file in this tutorial.)\n",
        "\n",
        "Let's begin by reading in and inspecting the dataset CSV:"
      ],
      "metadata": {
        "id": "MiuLLuZeL6mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A. EDA/Dataframe Operations**"
      ],
      "metadata": {
        "id": "GYkChQuEVASg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(DATA_DIR, \"input_data.csv\"))"
      ],
      "metadata": {
        "id": "X4G_ksiTMMv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "c2aJA0LXMUSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "r0XmFU_RMWci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "-0pyXF_PMYLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ekNDjTBSMeMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so it looks as though there are 106 samples, and 7 columns (features). 1 audio path is missing.\n",
        "\n",
        "What do you notice about the data structures for each column?\n",
        "\n",
        "The first necessary manipulation: now that the data dir lives under `/content`, \"content\" should be prepended to each `audio_path`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "br1uFHbFMyoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"audio_path\"] = df[\"audio_path\"].apply(lambda x: \"/content/\" + x if isinstance(x, str) else \"\")"
      ],
      "metadata": {
        "id": "kQTZqbrAMh3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GA3cf6w_Nxkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Blacklist Flag Column**\n",
        "\n",
        "This is just a repository for any \"warnings\" about entries\n",
        "that will assist with training data filtering downstream."
      ],
      "metadata": {
        "id": "vL4zcOhVR-8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"blacklist_flags\"] = [[] for _ in range(len(df))]"
      ],
      "metadata": {
        "id": "GxRtw8YWSaP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PsCq0H8aSeJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Add`_id` column**\n",
        "\n",
        "Assign each sample a unique  `_id`"
      ],
      "metadata": {
        "id": "5yZeg7wNVS-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"_id\"] = [str(uuid.uuid1()) for _ in range(len(df))]"
      ],
      "metadata": {
        "id": "jV_B5zEVVl1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "rxZiqP3cV1r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Convert \"genres\" values from string to list**"
      ],
      "metadata": {
        "id": "sjW2AV4Cj2kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"genres\"] = df[\"genres\"].apply(lambda x: ast.literal_eval(x))"
      ],
      "metadata": {
        "id": "By9fqsUbkEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Convert the Dataframe to a List of Dictionaries**\n",
        "\n",
        "For all subsequent operations, the data should be in dictionary format. (This also avoids dealing with the idiomatic quirks of pandas.)"
      ],
      "metadata": {
        "id": "FBj8kDrxT7jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = df.to_dict(orient=\"records\")"
      ],
      "metadata": {
        "id": "xzXKpVzzUgQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(entries)"
      ],
      "metadata": {
        "id": "JsP1noErXtgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entries[0]"
      ],
      "metadata": {
        "id": "C09kFfSrUxWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B. Data Validation and Pruning**\n",
        "\n",
        "As this preprocessing pipeline is positioned upstream, and given the limited dataset size, it would be best to take a conservative approach to making executive data filtering decisions.\n",
        "\n",
        "So let's consider: under which conditions is a given entry simply not usable as training data?\n",
        "\n",
        "\n",
        "1.   No audio filepath (remember: we found one such example)\n",
        "2.   Absolutely no relevant metadata\n",
        "3.   Duplicate audio path (put a pin in this for later...)"
      ],
      "metadata": {
        "id": "f8qO0kmvWMzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = validate_prune_data(entries)"
      ],
      "metadata": {
        "id": "IgtOCf5vXXTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(entries)} remaining entries\")"
      ],
      "metadata": {
        "id": "145MiynduAZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C. Duplicate Detection**\n",
        "\n",
        "Text metadata by itself cannot be fully trusted as the source of truth for data ontology, but the audio can (with a few caveats).\n",
        "\n",
        "There are multiple approaches to this--e.g., comparing hashes of audio file bytes, or taking the mean absolute distance between 2 audio arrays.\n",
        "\n",
        "Given the time complexity of this problem (comparing each audio file with every other audio file in the dataset is by default O(N^2), it would make sense to compare compact but rich representations of the audio. In this case, we will use Mel spectrogram embeddings, and compute cosine similarity.\n",
        "\n",
        "Step 1: extract and cache all embeddings\n",
        "\n",
        "\n",
        "Step 2: Iterate over inputs\n",
        "        \n",
        "\n",
        "*   if 2 entries point to the same audio filepath,\n",
        "mark one for deletion\n",
        "\n",
        "*    if 2 entries' similarity score exceeds a given threshold, flag one as a duplicate (but do not delete)\n",
        "\n",
        "\n",
        "Audio file duration is also computed at this stage.\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "OVGvms2iOMef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = find_similar_audio(entries)"
      ],
      "metadata": {
        "id": "RTyXy0siOecl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **D. Text Tokenization**\n",
        "\n",
        "Now that invalid samples have been removed, the focus can shift to text metadata. The first step is to tokenize all text input.\n",
        "\n",
        "Since the metadata for this dataset is already relatively tidy, the \"tokenization\" process in this case will merely consist of converting all text to lower case, and removing hyphens and related characters."
      ],
      "metadata": {
        "id": "ePcDxAzmO3dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = tokenize_metadata(entries)"
      ],
      "metadata": {
        "id": "bLEAen6wOjYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example entry:\n",
        "entries[0]"
      ],
      "metadata": {
        "id": "S1CTeTmHPC7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **E. Blacklisted Genre Extraction**\n",
        "\n",
        "Let's say that certain genre tags strongly correlate with either low-quality or irrelevant data (e.g., podcasts or environmental field recordings).\n",
        "\n",
        "For this stage, if any \"blacklisted\" genre tag is detected for a given entry, the flag `bad genre` will be appended to a list of `blacklist_flags`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cB-o3g4SPh_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = extract_blacklisted_genres(entries)"
      ],
      "metadata": {
        "id": "tqdbpbyfPGx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entries for which \"bad_genre\" flag exists:\n",
        "bad_genre_entries = [e for e in entries if \"bad_genre\" in e[\"blacklist_flags\"]]\n",
        "print(bad_genre_entries)"
      ],
      "metadata": {
        "id": "NQpCQcE2PuZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **F. Chunk/Segment Entries with Long Audio Tracks**\n",
        "It is not uncommon in certain genres for individual tracks to extend for long durations (e.g., the movement of a symphony, or a meditation track). This poses practical issues in the model training context.\n",
        "\n",
        "For any samples whose audio duration exceeds a given threshold, the audio will be segmented into subfiles which will be saved to the audio data directory.\n",
        "\n",
        "For each segment, the \"source\" entry metadata will be copied, but the audio path and duration will be overwritten. In addition, the source entry `_id ` and partition index will be added.\n",
        "\n",
        "(Do you see why this step follows text metadata preprocessing?)"
      ],
      "metadata": {
        "id": "UoIa0Tc7QA61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = chunk_audio(entries)"
      ],
      "metadata": {
        "id": "A6v9MuprQJ7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entries[-6:]"
      ],
      "metadata": {
        "id": "-FSwuzP3QgJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **G. Silent Region Detection**\n",
        "Given that the model will be trained on (random) crops of a particular duration, it would be best to avoid exposing it to segments that consist primarily of silence.\n",
        "\n",
        "As such, silent regions are detected, collected, and logged for each entry. (A silent region is defined as an inter-onset interval > a given threshold.)\n",
        "\n",
        "(Do you see why this step follows audio segmentation?)  \n",
        "\n",
        "As for how these regions are handled: this will be outsourced to downstream stages..."
      ],
      "metadata": {
        "id": "RUXvOZW4Qnhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries = add_silent_regions(entries)"
      ],
      "metadata": {
        "id": "yHrepns9QvHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silent_region_entries = [e for e in entries if e[\"silent_regions\"]]\n",
        "len(silent_region_entries)"
      ],
      "metadata": {
        "id": "5saBx4l7Q_km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silent_region_entries"
      ],
      "metadata": {
        "id": "-irylL23RF96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **H. Serialize to JSON**"
      ],
      "metadata": {
        "id": "9JiUKe74RThu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/data/training_data.json\", \"w\") as f:\n",
        "  json.dump(entries, f, indent=4)"
      ],
      "metadata": {
        "id": "66Q3Zko1RhHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Run this cell in case of runtime disconnection, etc., or just to validate serialization):\n",
        "with open(\"/content/data/training_data.json\", \"r\") as f:\n",
        "  entries = json.load(f)"
      ],
      "metadata": {
        "id": "JdYeMbZwNkeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pipeline TODOs:**\n",
        "\n",
        "**Audio**\n",
        "\n",
        "\n",
        "1.   Music vs. non-music regions/ratio\n",
        "2.   Instrumental vs. vocal regions\n",
        "3.   ?Audio fingerprinting?\n",
        "\n",
        "**Text**\n",
        "1. Filtering tags by top genres/artists (keep only the most frequent)\n",
        "2. Named entity resolution (e.g., 2+ renderings of same artist or genre name)\n",
        "3. Genre name \"smoothing\" (i.e., mapping multiple genre names to 1 meta-category)\n",
        "4. ?API pinging for additional metadata?\n",
        "\n"
      ],
      "metadata": {
        "id": "YwoNqzADezc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II. Audio Dataset/DataLoader**\n",
        "\n",
        "Now that we have preprocessed the training data, the next step is to determine how data samples will be varied, transformed, and represented during model training and validation."
      ],
      "metadata": {
        "id": "YSKDJsoTLLAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A. AudioDataset**\n",
        "Let's construct an AudioDataset. Each `__getitem__()` call returns a waveform tensor and a `TextCondition`.\n",
        "\n",
        " The only required argument is the list of entries (training data collection).\n"
      ],
      "metadata": {
        "id": "pQlOvSOJP0lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = AudioDataset(entries)"
      ],
      "metadata": {
        "id": "NHJXGo8_Rm8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To inspect output properties, let's print the shape of the audio tensor and the `TextCondition` for one `__getitem__()` call. (Note that audio is mixed down to mono, as the dataset *may* contain a combination of mono and stereo files.)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQ3V7cS-QrD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio, cond = ds[0]\n",
        "print(f\"Audio tensor shape: {audio.shape}, Text condition: {cond}\")"
      ],
      "metadata": {
        "id": "tmP8zmp1RnMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a script in tests called `test_dataset.py`, which iterates thrrough the dataset, and prints/collects any errors, but we can )(partially) replicate the functionality here:"
      ],
      "metadata": {
        "id": "UAnU86nAlEkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(ds: Dataset, max_idx = 10) -> None:\n",
        "  \"\"\"\n",
        "  Iterates through dataset. Prints audio shapes and text conditions. If there are any\n",
        "  exceptions thrown, prints the exception.\n",
        "  \"\"\"\n",
        "  for i in range(max_idx):\n",
        "    try:\n",
        "      audio, cond = ds[i]\n",
        "      print(f\"{i}: Audio tensor shape: {audio.shape}, Text condition: {cond}\")\n",
        "      if audio.shape[0] != 1:\n",
        "        print(f\"n_channel mismatch at index {i}: should be mono.\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "  print(\"Dataset tests passed!\")"
      ],
      "metadata": {
        "id": "tofIeF07lPkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_dataset(ds)"
      ],
      "metadata": {
        "id": "6haqF6QCmd8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B. AudioDataset with Transform**\n",
        "\n",
        "There is an option to apply an audio transform to the input. Let's experiment with an MFCC transform."
      ],
      "metadata": {
        "id": "WS-xWewJnjeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_transform = MFCC(\n",
        "  sample_rate=44100,\n",
        "  n_mfcc=13,\n",
        "  melkwargs={\"n_fft\": 2048, \"hop_length\": 512, \"n_mels\": 23}\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "o0npqvdnpglx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_ds = AudioDataset(entries, transform=mfcc_transform)"
      ],
      "metadata": {
        "id": "ksHtzV6qu2QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_spect, cond = mfcc_ds[0]\n",
        "print(f\"Audio tensor shape: {mfcc_spect.shape}, Text condition: {cond}\")"
      ],
      "metadata": {
        "id": "eqqCftPRvAKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset TODOs:**\n",
        "\n",
        "\n",
        "1.   Support silent region-aware cropping\n",
        "2.   Chain augmentations (i.e., select > 1)\n",
        "3.   ?Other audio representations?\n",
        "\n",
        "\n",
        "*   Discrete (precomputed?) codebooks?\n",
        "*   Continuous or discretized embeddings? (Precomputed, or extracted on-the-fly?)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yJJiqmTpaG_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C. Train/Test Split**\n",
        "The Dataset will be split in train and validation subsets. (Be sure that sample are randomly selected for each.)\n"
      ],
      "metadata": {
        "id": "5i-l31xlTqUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(ds: Dataset, split_ratio: float = 0.8) -> Tuple[Subset, Subset]:\n",
        "  \"\"\"\n",
        "  Splits a Dataset into training and validation subsets.\n",
        "  \"\"\"\n",
        "  all_idxs = range(len(ds))\n",
        "  train_len = int(0.8 * len(ds))\n",
        "  rand_idxs = random.sample(range(len(ds)), k=train_len)\n",
        "  train_ds = Subset(ds, rand_idxs)\n",
        "\n",
        "  val_idxs = list(set(all_idxs) - set(rand_idxs))\n",
        "  val_ds = Subset(ds, val_idxs)\n",
        "\n",
        "  return train_ds, val_ds\n"
      ],
      "metadata": {
        "id": "SINAzCysvCw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds = train_test_split(ds)"
      ],
      "metadata": {
        "id": "c9b9zb4kXjai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")"
      ],
      "metadata": {
        "id": "M8dYUna13ENh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **D. DataLoaders**\n",
        "\n",
        "In order to transfer data efficiently from CPU (and not load all data into RAM), a `DataLoader` is recommended.\n",
        "\n",
        "Let's create one for each `Dataset`."
      ],
      "metadata": {
        "id": "zHA9QDT3dqC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collate function**\n",
        "\n",
        "For correct data batching, a `collate_fn` is necessary to define. As the audio = tensors of uniform dimensions, these can be stacked. The `TextConditions`, are not tensors, however, and should just be concatenated to a list:"
      ],
      "metadata": {
        "id": "Mn2i9kdUfEDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    audio, conditions = zip(*data)\n",
        "    return torch.stack(audio), list(conditions)"
      ],
      "metadata": {
        "id": "p3_nOUOqfjSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Validation DataLoaders**\n",
        "\n",
        "The main difference between the training and validation DataLoader kwargs is that, for the validation set, shuffling data is not required. For the purposes of Colab notebook execution, be sure to set `num_workers` to 0.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L5R7-iCiftnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=4, collate_fn=collate_fn, num_workers=0, pin_memory=False, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=4, collate_fn=collate_fn, num_workers=0, pin_memory=False, shuffle=False)"
      ],
      "metadata": {
        "id": "QiHx-qsPhiZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataloader(dl: DataLoader, max_iter: int = 5) -> None:\n",
        "  for i in range(max_iter):\n",
        "    audio_batch, cond_batch = next(iter(dl))\n",
        "    print(f\"Audio dims: {audio_batch.shape}, Text batch size: {len(cond_batch)}\")\n",
        "\n",
        "  print(\"DataLoader tests passed!\")"
      ],
      "metadata": {
        "id": "pQ2p6_jPhrjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_dataloader(val_dl)"
      ],
      "metadata": {
        "id": "INrCkvhFicVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion/Next Steps**\n",
        "\n",
        "With the initialized DataLoaders, one could run a model training/validation loop over some number of epochs, reading samples from CPU in batche and transferring to GPU.\n",
        "\n",
        "For each epoch, for a given sample, a unique crop and augmentation type will be selected, thereby both expanding the training dataset and teaching the model to generate to and from any valid onset, and be robust to imperceptible (or scarcely perceptible) but computationally distinct representations of the \"same\" input. (As discussed earlier, the applied augmentations should not degrade the audio samples--this is \"ground truth\" audio, and the model should be steered towards generating high-quality outputs.)\n",
        "\n",
        "In the \"real world\", the dataset would be vastly larger (in most cases, at least 20K hours ++), with greater variance in input duration and data source, but the data pipeline stages/mechanics and Dataset design process covered in this tutorial would be fundamentally equivalent.\n",
        "\n",
        "Please feel free to experiment with your own data, pipeline enhancements, and Dataset modifications!"
      ],
      "metadata": {
        "id": "jyLSPodM4fuK"
      }
    }
  ]
}